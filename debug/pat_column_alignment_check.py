import os
import pandas as pd
from openpyxl import load_workbook
from collections import defaultdict
import re

def analyze_column_meanings(municipality_name="ä¸æ•´åˆãƒ†ã‚¹ãƒˆè‡ªæ²»ä½“v3"):
    """
    PATé–“ã§ã®åˆ—ã®æ„å‘³çš„ã‚ºãƒ¬ã‚’æ¤œå‡ºãƒ»åˆ†æã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
    """
    base_dir = os.path.join(
        r'G:\å…±æœ‰ãƒ‰ãƒ©ã‚¤ãƒ–\â˜…OD\99_å•†å“ç®¡ç†\DATA\Phase3\HARV',
        municipality_name
    )
    
    if not os.path.exists(base_dir):
        print(f"æŒ‡å®šãƒ•ã‚©ãƒ«ãƒ€ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {base_dir}")
        return
    
    # å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆé‡è¤‡æ­£è¦åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã¯é™¤å¤–ï¼‰
    files = [f for f in os.listdir(base_dir) 
             if f.startswith("PAT") and f.endswith("_normalized.xlsx") 
             and not f.endswith("_normalized_normalized.xlsx")]
    
    if not files:
        print("å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    
    print(f"åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}å€‹")
    print("=" * 100)
    
    # å„PATãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±ã‚’åé›†
    pat_headers = {}
    pat_sample_data = {}
    
    for file in sorted(files):
        file_path = os.path.join(base_dir, file)
        print(f"\n=== {file} ã®åˆ†æ ===")
        
        try:
            wb = load_workbook(file_path, data_only=True)
            ws = wb.active
            max_row = ws.max_row
            max_col = ws.max_column

            # ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆå…¨ä½“ã®å€¤ã‚’2æ¬¡å…ƒãƒªã‚¹ãƒˆã«ã‚³ãƒ”ãƒ¼
            data = []
            for row in ws.iter_rows(min_row=1, max_row=max_row, min_col=1, max_col=max_col):
                data.append([cell.value for cell in row])

            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆBåˆ—ãŒã€Œé …ç›®ã€ï¼‰ã‚’æ¤œç´¢
            header_row_index = None
            for i, row in enumerate(data):
                if len(row) >= 2 and row[1] == "é …ç›®":
                    header_row_index = i
                    break

            if header_row_index is None:
                print(f"  âš ï¸ ã€Œé …ç›®ã€è¡ŒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                continue

            # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æŠ½å‡º
            headers = [cell for cell in data[header_row_index][2:] if cell is not None]
            print(f"  ğŸ“‹ ãƒ˜ãƒƒãƒ€ãƒ¼æ•°: {len(headers)}")
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆãƒ‡ãƒ¼ã‚¿è¡Œã®æœ€åˆã®æ•°è¡Œï¼‰
            sample_data = []
            data_start = header_row_index + 1
            for row_idx in range(data_start, min(data_start + 5, len(data))):
                if row_idx < len(data):
                    row_data = data[row_idx][2:]  # Cåˆ—ä»¥é™
                    sample_data.append(row_data)
            
            pat_headers[file] = headers
            pat_sample_data[file] = sample_data
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä¸€éƒ¨ã‚’è¡¨ç¤º
            print(f"  ğŸ“ ãƒ˜ãƒƒãƒ€ãƒ¼ä¾‹ï¼ˆæœ€åˆã®10å€‹ï¼‰:")
            for i, header in enumerate(headers[:10]):
                print(f"    [{i+3}åˆ—ç›®] {header}")
            
        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
    
    print("\n" + "=" * 100)
    print("=== PATé–“ã§ã®ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®æ¯”è¼ƒ ===")
    
    # å…±é€šãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç‰¹å®š
    all_headers = set()
    for headers in pat_headers.values():
        all_headers.update(headers)
    
    common_headers = []
    for header in all_headers:
        count = sum(1 for headers in pat_headers.values() if header in headers)
        if count >= 2:  # 2ã¤ä»¥ä¸Šã®PATã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼
            common_headers.append((header, count))
    
    # ä½¿ç”¨é »åº¦é †ã§ã‚½ãƒ¼ãƒˆ
    common_headers.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\nğŸ“Š è¤‡æ•°PATã§ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼: {len(common_headers)}å€‹")
    
    # ä½ç½®ã‚ºãƒ¬ã®æ¤œå‡º
    alignment_issues = []
    
    for header, usage_count in common_headers[:20]:  # ä¸Šä½20å€‹ã‚’ãƒã‚§ãƒƒã‚¯
        positions = {}
        for file, headers in pat_headers.items():
            if header in headers:
                pos = headers.index(header)
                positions[file] = pos
        
        if len(set(positions.values())) > 1:  # ä½ç½®ãŒç•°ãªã‚‹å ´åˆ
            alignment_issues.append((header, positions))
    
    print(f"\nğŸš¨ ä½ç½®ã‚ºãƒ¬ãŒæ¤œå‡ºã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼: {len(alignment_issues)}å€‹")
    
    # è©³ç´°åˆ†æ
    critical_issues = []
    
    for header, positions in alignment_issues:
        print(f"\n--- ãƒ˜ãƒƒãƒ€ãƒ¼: ã€Œ{header}ã€ ---")
        
        # å„PATã§ã®ä½ç½®ã¨ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
        sample_values = {}
        for file, pos in positions.items():
            print(f"  {file}: {pos+3}åˆ—ç›®ï¼ˆCåˆ—ã‚’3åˆ—ç›®ã¨ã™ã‚‹ï¼‰")
            
            # ãã®ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            if file in pat_sample_data:
                values = []
                for sample_row in pat_sample_data[file]:
                    if pos < len(sample_row) and sample_row[pos] is not None:
                        val = str(sample_row[pos]).strip()
                        if val and val != "":
                            values.append(val)
                
                sample_values[file] = values[:3]  # æœ€åˆã®3å€‹
                print(f"    ã‚µãƒ³ãƒ—ãƒ«: {values[:3]}")
        
        # ãƒ‡ãƒ¼ã‚¿ã®å†…å®¹ã‚’æ¯”è¼ƒã—ã¦æ„å‘³çš„ãªé•ã„ã‚’æ¤œå‡º
        is_critical = detect_semantic_mismatch(header, sample_values)
        if is_critical:
            critical_issues.append((header, positions, sample_values))
    
    print("\n" + "=" * 100)
    print("=== ğŸ”¥ é‡å¤§ãªåˆ—ã‚ºãƒ¬å•é¡Œ ===")
    
    if critical_issues:
        for i, (header, positions, sample_values) in enumerate(critical_issues, 1):
            print(f"\n{i}. ã€{header}ã€‘")
            print("  ç•°ãªã‚‹æ„å‘³ã®ãƒ‡ãƒ¼ã‚¿ãŒåŒã˜ãƒ˜ãƒƒãƒ€ãƒ¼ã«çµ±åˆã•ã‚Œã¦ã„ã¾ã™ï¼š")
            for file, values in sample_values.items():
                pos = positions[file]
                print(f"    {file} ({pos+3}åˆ—ç›®): {values}")
            
            print("  ğŸ’¡ æ¨å¥¨å¯¾å¿œ:")
            print("    - å„PATãƒ•ã‚¡ã‚¤ãƒ«ã§ã®ãƒ˜ãƒƒãƒ€ãƒ¼åã‚’çµ±ä¸€")
            print("    - ã¾ãŸã¯ã€ãƒ˜ãƒƒãƒ€ãƒ¼ä½ç½®ã‚’æƒãˆã‚‹")
    else:
        print("é‡å¤§ãªæ„å‘³çš„ã‚ºãƒ¬ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    print(f"\nğŸ“ˆ åˆ†æã‚µãƒãƒªãƒ¼:")
    print(f"  - åˆ†æå¯¾è±¡PATæ•°: {len(pat_headers)}")
    print(f"  - å…±é€šãƒ˜ãƒƒãƒ€ãƒ¼æ•°: {len(common_headers)}")
    print(f"  - ä½ç½®ã‚ºãƒ¬ãƒ˜ãƒƒãƒ€ãƒ¼æ•°: {len(alignment_issues)}")
    print(f"  - é‡å¤§å•é¡Œæ•°: {len(critical_issues)}")
    
    # è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’CSVã§å‡ºåŠ›
    save_detailed_report(base_dir, pat_headers, alignment_issues, sample_values, municipality_name)

def detect_semantic_mismatch(header, sample_values):
    """
    ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ„å‘³çš„ãªä¸ä¸€è‡´ã‚’æ¤œå‡º
    """
    if len(sample_values) < 2:
        return False
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®åˆ†æ
    types_analysis = {}
    for file, values in sample_values.items():
        analysis = analyze_data_type(values)
        types_analysis[file] = analysis
    
    # ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ãŒæ··åœ¨ã—ã¦ã„ã‚‹å ´åˆã¯é‡å¤§
    unique_types = set()
    for analysis in types_analysis.values():
        unique_types.add(analysis['primary_type'])
    
    if len(unique_types) > 1:
        print(f"    ğŸš¨ ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ã®ä¸ä¸€è‡´æ¤œå‡º: {unique_types}")
        return True
    
    # ç‰¹å®šã®äººåãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å•†å“åãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ··åœ¨ãƒã‚§ãƒƒã‚¯
    has_person_name = False
    has_product_name = False
    
    for values in sample_values.values():
        for val in values:
            if is_likely_person_name(val):
                has_person_name = True
            if is_likely_product_name(val):
                has_product_name = True
    
    if has_person_name and has_product_name:
        print(f"    ğŸš¨ äººåã¨å•†å“åã®æ··åœ¨æ¤œå‡º")
        return True
    
    return False

def analyze_data_type(values):
    """
    ãƒ‡ãƒ¼ã‚¿ã®ç¨®é¡ã‚’åˆ†æ
    """
    if not values:
        return {'primary_type': 'empty'}
    
    numeric_count = 0
    date_count = 0
    text_count = 0
    
    for val in values:
        val_str = str(val).strip()
        if not val_str:
            continue
            
        # æ•°å€¤ãƒã‚§ãƒƒã‚¯
        if re.match(r'^\d+$', val_str) or re.match(r'^\d+\.\d+$', val_str):
            numeric_count += 1
        # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
        elif re.match(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', val_str):
            date_count += 1
        # ãã®ä»–ã¯ãƒ†ã‚­ã‚¹ãƒˆ
        else:
            text_count += 1
    
    total = numeric_count + date_count + text_count
    if total == 0:
        return {'primary_type': 'empty'}
    
    if numeric_count / total > 0.6:
        return {'primary_type': 'numeric', 'confidence': numeric_count / total}
    elif date_count / total > 0.6:
        return {'primary_type': 'date', 'confidence': date_count / total}
    else:
        return {'primary_type': 'text', 'confidence': text_count / total}

def is_likely_person_name(value):
    """
    äººåã‚‰ã—ã„ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    """
    if not value:
        return False
    
    val = str(value).strip()
    
    # æ—¥æœ¬äººåã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    if re.match(r'^[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]{2,6}[\sã€€]*[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]{1,6}$', val):
        return True
    
    # ã€Œæ²³ç€¨ã€€é€ã€ã®ã‚ˆã†ãªå…·ä½“ä¾‹
    person_patterns = ['æ²³ç€¨', 'é€', 'ç”°ä¸­', 'ä½è—¤', 'å±±ç”°', 'é«˜æ©‹', 'æ¾æœ¬']
    return any(pattern in val for pattern in person_patterns)

def is_likely_product_name(value):
    """
    å•†å“åã‚‰ã—ã„ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    """
    if not value:
        return False
    
    val = str(value).strip()
    
    # å•†å“åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
    product_keywords = ['kg', 'g', 'ml', 'L', 'å€‹', 'ã‚»ãƒƒãƒˆ', 'è©°åˆã›', 'ç‰›è‚‰', 'ç±³', 'ã¿ã‹ã‚“', 'ã„ã¡ã”']
    return any(keyword in val for keyword in product_keywords)

def save_detailed_report(base_dir, pat_headers, alignment_issues, sample_values, municipality_name):
    """
    è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã§ä¿å­˜
    """
    import csv
    
    report_path = os.path.join(base_dir, f"{municipality_name}_column_alignment_report.csv")
    
    with open(report_path, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
        writer.writerow(['ãƒ˜ãƒƒãƒ€ãƒ¼å', 'PAT1ãƒ•ã‚¡ã‚¤ãƒ«', 'PAT1ä½ç½®', 'PAT1ã‚µãƒ³ãƒ—ãƒ«', 
                        'PAT2ãƒ•ã‚¡ã‚¤ãƒ«', 'PAT2ä½ç½®', 'PAT2ã‚µãƒ³ãƒ—ãƒ«', 'å•é¡Œãƒ¬ãƒ™ãƒ«'])
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for header, positions in alignment_issues:
            files = list(positions.keys())
            if len(files) >= 2:
                file1, file2 = files[0], files[1]
                pos1, pos2 = positions[file1], positions[file2]
                
                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—
                sample1 = sample_values.get(file1, []) if 'sample_values' in locals() else []
                sample2 = sample_values.get(file2, []) if 'sample_values' in locals() else []
                
                sample1_str = ' | '.join(map(str, sample1[:2]))
                sample2_str = ' | '.join(map(str, sample2[:2]))
                
                # å•é¡Œãƒ¬ãƒ™ãƒ«åˆ¤å®š
                level = "é«˜" if detect_semantic_mismatch(header, {file1: sample1, file2: sample2}) else "ä¸­"
                
                writer.writerow([header, file1, pos1+3, sample1_str, 
                                file2, pos2+3, sample2_str, level])
    
    print(f"\nğŸ“„ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {report_path}")

if __name__ == "__main__":
    analyze_column_meanings()
